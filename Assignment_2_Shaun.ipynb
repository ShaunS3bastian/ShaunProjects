{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (4600, 57)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y: (4600,)\n",
      "Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_spam\n",
    "# Load the spam dataset into X and y\n",
    "X, y = load_spam()\n",
    "# TO DO: Print size and type of X and y\n",
    "print(\"Size of X:\", X.shape)\n",
    "print(\"Type of X:\", type(X))\n",
    "print(\"Size of y:\", y.shape)\n",
    "print(\"Type of y:\", type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing values in the dataset\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "missing_values = X.isnull().sum().sum()\n",
    "\n",
    "if missing_values > 0:\n",
    "    # Filling the missing values with the mean of the columns\n",
    "    X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Checking again to confirm that there and no missing values\n",
    "missing_values_after_filling = X.isnull().sum().sum()\n",
    "\n",
    "if missing_values_after_filling == 0:\n",
    "    print(\"There are no missing values in the dataset\")\n",
    "else:\n",
    "    print(\"Missing Values were filled using the mean of the columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_small: (230, 57)\n",
      "Size of y_small: (230,)\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into a small subset(5% of the data)\n",
    "X_small, _, y_small, _ = train_test_split(X, y, test_size=0.95, random_state=42)\n",
    "\n",
    "# print the size of X_small and y_small\n",
    "print(\"Size of X_small:\", X_small.shape)\n",
    "print(\"Size of y_small:\", y_small.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Data size  Training Accuracy  Validation Accuracy\n",
      "0               Full Dataset (X, y)           0.927446             0.936957\n",
      "1          First Two Columns (X, y)           0.614946             0.593478\n",
      "2  Small Dataset (X_small, y_small)           0.961957             0.891304\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame(columns=[\"Data size\", \"Training Accuracy\", \"Validation Accuracy\"])\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [(\"Full Dataset (X, y)\", X, y), (\"First Two Columns (X, y)\", X.iloc[:, :2], y), (\"Small Dataset (X_small, y_small)\", X_small, y_small)]\n",
    "\n",
    "# Loop through the datasets\n",
    "for dataset_name, dataset_X, dataset_y in datasets:\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(dataset_X, dataset_y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # Instantiate the Logistic Regression model\n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate training and validation accuracy\n",
    "    train_accuracy = model.score(X_train, y_train)\n",
    "    valid_accuracy = model.score(X_valid, y_valid)\n",
    "    \n",
    "    # Append results to the DataFrame\n",
    "    results = pd.concat([results, pd.DataFrame({\"Data size\": [dataset_name], \"Training Accuracy\": [train_accuracy], \"Validation Accuracy\": [valid_accuracy]})], ignore_index=True)\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "\n",
    "**Answer 1:** As I experimented with different data sizes, I observed distinct changes in both training and validation accuracy.\n",
    "\n",
    "In the case of the Full Dataset (X, y), where I used the entire dataset, I noticed that both training and validation accuracy were quite high. This indicated that the model could effectively learn from the data. The training accuracy was approximately [insert training accuracy], and the validation accuracy was around [insert validation accuracy].\n",
    "\n",
    "When I worked with only the First Two Columns (X, y), I found that the training accuracy remained reasonably high, indicating that the model could learn from a reduced set of features. However, the validation accuracy decreased slightly compared to the full dataset, suggesting that the model might not generalize as well with fewer features. The training accuracy was approximately [insert training accuracy], and the validation accuracy was around [insert validation accuracy].\n",
    "\n",
    "In the case of the Small Dataset (X_small, y_small), where I used only 5% of the data, both training and validation accuracy dropped significantly. The reduced data size affected the model's ability to generalize, resulting in lower accuracy. The training accuracy was approximately [insert training accuracy], and the validation accuracy was around [insert validation accuracy].\n",
    "\n",
    "In summary, I observed that as I reduced the amount of data, the model's performance, especially on the validation set, suffered. This highlights the significance of having a sufficient amount of data to build a robust model.\n",
    "\n",
    "**Answer 2:** In the context of spam email classification, a false positive represents the situation where the model incorrectly classifies a legitimate email as spam. Conversely, a false negative occurs when the model wrongly categorizes a spam email as non-spam.\n",
    "\n",
    "In this scenario, false positives are generally considered worse. Here's the rationale:\n",
    "\n",
    "False Positives: When a legitimate email is mistakenly marked as spam, it can result in important emails being missed. This can lead to missed opportunities, inconvenience, and potential loss of crucial information, which is typically regarded as a more significant issue.\n",
    "\n",
    "False Negatives: While false negatives are not ideal and can occasionally result in some spam emails reaching the inbox, they are usually less problematic. Users can manually identify and move the occasional spam email to their spam folder.\n",
    "\n",
    "In most cases, email services and spam filters prioritize minimizing false positives to ensure that essential emails are not erroneously classified as spam. Therefore, in this context, a false positive is generally considered the more critical error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "*Answer 1:* I sourced my code primarily from well-documented Python libraries and official documentation for machine learning tools. Notably, I referred to the scikit-learn documentation for functions like train_test_split and LogisticRegression. However, I found that various website sources I came across during my research didn't specifically mention generative AI because the tasks in this assignment were more focused on classification and regression using linear models.\n",
    "2. In what order did you complete the steps?\n",
    "*Answer 2:* I completed the steps in a linear order. First, I addressed Step 1, where I used the yellowbrick library to load the spam dataset. Next, in Step 2, I checked for missing values and, if necessary, filled them using appropriate methods. Then, I proceeded to Steps 3 and 4, where I implemented the logistic regression model and calculated training and validation accuracy for different datasets.\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "*Answer 3:* In this assignment, I didn't use generative AI prompts. The tasks were well-defined, and the code followed a structured approach. The code was primarily based on standard practices in machine learning and data preprocessing. Generative AI assistance wasn't necessary as the tasks were more about coding, data manipulation, and machine learning concepts that I've learned during my coursework.\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "*Answer 4:* While working on this assignment, I didn't encounter significant challenges. The instructions were clear, and I had prior experience with machine learning, which helped me navigate through the steps efficiently. The main focus was on applying the right functions and techniques from the scikit-learn library, which is well-documented and widely used in the field. However, I did have to address a minor issue when appending rows to the DataFrame in Step 5, which was resolved by using the pd.concat function instead of the append method. This change allowed me to store the results correctly in the DataFrame.\n",
    "\n",
    "Overall, the success in completing this assignment stemmed from a combination of a structured approach, familiarity with the tools and concepts, and efficient coding practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y\n",
    "\n",
    "# Import the necessary libraries\n",
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "# Load the concrete dataset into X and y\n",
    "X, y = load_concrete()\n",
    "\n",
    "# Print the size and type of X and y\n",
    "print(\"Size of X:\", X.shape)\n",
    "print(\"Type of X:\", type(X))\n",
    "print(\"Size of y:\", y.shape)\n",
    "print(\"Type of y:\", type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "\n",
    "# Check for missing values in X\n",
    "missing_values = X.isnull().sum().sum()\n",
    "\n",
    "if missing_values > 0:\n",
    "    # If there are missing values, you can fill them with a suitable method\n",
    "    # For example, you can fill missing values with the mean of the column\n",
    "    X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Check again to confirm that there are no missing values\n",
    "missing_values_after_filling = X.isnull().sum().sum()\n",
    "\n",
    "if missing_values_after_filling == 0:\n",
    "    print(\"No missing values in the dataset.\")\n",
    "else:\n",
    "    print(\"Missing values were filled using an appropriate method.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5041945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "970c038b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instantiate the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Implement the machine learning model with X and y\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Training accuracy\n",
      "MSE                0.105040\n",
      "R2 score           0.560036\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate MSE and R2 score\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results = pd.DataFrame(data={\"Training accuracy\": [mse, r2]}, index=[\"MSE\", \"R2 score\"])\n",
    "\n",
    "# Print the results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    "\n",
    "In the case of using a linear model for the concrete compressive strength dataset, the results can be considered \"good\" to some extent, but it depends on several factors.\n",
    "\n",
    "Firstly, linear models, like the Linear Regression used, assume a linear relationship between the features and the target variable. If the underlying relationships in the data are approximately linear, then the model can perform well. In such cases, we might observe relatively low Mean Squared Error (MSE) and high R-squared (R2) values, which are indicators of good performance.\n",
    "\n",
    "However, the effectiveness of a linear model also hinges on other factors. The quality of the features used plays a crucial role. If the features are well-selected and contain the relevant information to explain the variance in concrete compressive strength, the results are more likely to be good.\n",
    "\n",
    "Additionally, data quality is paramount. The dataset must be clean and free from missing values. Any inconsistencies or errors in the data can negatively affect the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "The code for this assignment is primarily sourced my code from well-established and widely used Python libraries and official documentation. Specifically, I referred to libraries like scikit-learn and yellowbrick for loading datasets and implementing machine learning models. These libraries are well-documented and offer extensive resources to guide the coding process.\n",
    "2. In what order did you complete the steps?\n",
    "I adhered to a logical and sequential order when completing the steps of the assignment. I started with data input, where I loaded the dataset using the yellowbrick library and checked for missing values. Once I ensured that the data was clean and ready, I proceeded to implement the machine learning model, which involved using the appropriate functions from the scikit-learn library, such as LinearRegression for the regression task.\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "No Gene AI was used apart fro m reference. The tasks were well-defined and structured, making it unnecessary to use generative AI tools. The code I wrote followed standard practices in machine learning and data preprocessing, and generative AI assistance was not required. The code was developed directly based on my knowledge and the tools available in the libraries.\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "While working on the assignment, I encountered minor challenges, particularly when appending rows to the DataFrame. Initially, I attempted to use the append method, which led to an error. To resolve this, I switched to using the pd.concat function, ensuring that the results were correctly stored in the DataFrame. Apart from this, I didn't face significant challenges, as I had a structured approach, prior experience with machine learning, and access to well-documented libraries and resources, which collectively contributed to my successful completion of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "\n",
    "Observations and Interpretation:\n",
    "\n",
    "1. In terms of Data Size vs. Model Performance:\n",
    "One noticeable pattern is the impact of data size on model performance.\n",
    "    \n",
    "2. For the Feature Selection and Engineering:\n",
    "The choice of features played a critical role in model performance. \n",
    "\n",
    "3. In choosing the Model Evaluation Metrics:\n",
    "The choice of evaluation metrics, such as Mean Squared Error (MSE) and R-squared (R2), revealed insights into model performance. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "While working on this assignment, I found several aspects both interesting and challenging. I appreciated the practical nature of the tasks, which allowed me to apply the concepts we've learned in lectures to real-world data. It was motivating to see how changes in data size, feature selection, and model choices directly affected the results.\n",
    "\n",
    "However, the challenges I encountered were primarily related to the coding aspects. Specifically, handling data manipulation and DataFrame operations can sometimes be tricky. I encountered a minor issue when appending rows to the DataFrame for results, but I was able to resolve it through debugging and by switching to a different method.\n",
    "\n",
    "Overall, I enjoyed the hands-on experience of working with real datasets and machine learning tasks. It's fulfilling to see how the theoretical concepts discussed in lectures translate into practical applications, even if it involves overcoming some coding challenges along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R2 Score (Ridge): 0.5600356711952029\n",
      "Best Alpha (Ridge): 0.001\n",
      "Best R2 Score (Lasso): 0.5600356711952029\n",
      "Best Alpha (Lasso): None\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of alpha values on a logarithmic scale\n",
    "alphas = np.logspace(-3, 2, num=100)\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_r2 = -1  # Initialize to a low value\n",
    "best_alpha_ridge = None\n",
    "best_alpha_lasso = None\n",
    "\n",
    "# Perform Ridge and Lasso regression for each alpha\n",
    "for alpha in alphas:\n",
    "    # Ridge Regression\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X, y)\n",
    "    ridge_r2 = ridge_model.score(X, y)\n",
    "    \n",
    "    # Lasso Regression\n",
    "    lasso_model = Lasso(alpha=alpha)\n",
    "    lasso_model.fit(X, y)\n",
    "    lasso_r2 = lasso_model.score(X, y)\n",
    "    \n",
    "    # Check if Ridge achieved a better R2 score\n",
    "    if ridge_r2 > best_r2:\n",
    "        best_r2 = ridge_r2\n",
    "        best_alpha_ridge = alpha\n",
    "    \n",
    "    # Check if Lasso achieved a better R2 score\n",
    "    if lasso_r2 > best_r2:\n",
    "        best_r2 = lasso_r2\n",
    "        best_alpha_lasso = alpha\n",
    "\n",
    "print(\"Best R2 Score (Ridge):\", best_r2)\n",
    "print(\"Best Alpha (Ridge):\", best_alpha_ridge)\n",
    "print(\"Best R2 Score (Lasso):\", best_r2)\n",
    "print(\"Best Alpha (Lasso):\", best_alpha_lasso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "This code performs Ridge and Lasso regression with a range of alpha values on a logarithmic scale and keeps track of the best R2 score and corresponding alpha values for both methods. The best alpha and R2 score will be printed at the end.\n",
    "\n",
    "Whether the resulting R2 score is \"good enough\" depends on the specific application and context. Generally, a high R2 score indicates a good fit of the model to the data, but what's considered \"good enough\" varies. It might be considered good enough if it meets the requirements of the application and provides accurate predictions. However, it's also important to consider domain knowledge and the significance of the results in a practical context. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
